{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:47:52.218072Z","iopub.execute_input":"2025-08-09T07:47:52.218253Z","iopub.status.idle":"2025-08-09T07:47:52.746361Z","shell.execute_reply.started":"2025-08-09T07:47:52.218236Z","shell.execute_reply":"2025-08-09T07:47:52.745584Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:47:56.984653Z","iopub.execute_input":"2025-08-09T07:47:56.985108Z","iopub.status.idle":"2025-08-09T07:49:30.592193Z","shell.execute_reply.started":"2025-08-09T07:47:56.985084Z","shell.execute_reply":"2025-08-09T07:49:30.591408Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from fastai.vision.all import *\nfrom fastbook import *\nfrom fastai.text.all import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:49:35.236010Z","iopub.execute_input":"2025-08-09T07:49:35.236555Z","iopub.status.idle":"2025-08-09T07:49:35.269926Z","shell.execute_reply.started":"2025-08-09T07:49:35.236525Z","shell.execute_reply":"2025-08-09T07:49:35.268454Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = untar_data(URLs.IMDB)\n!ls {path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:49:39.612016Z","iopub.execute_input":"2025-08-09T07:49:39.612288Z","iopub.status.idle":"2025-08-09T07:50:05.144487Z","shell.execute_reply.started":"2025-08-09T07:49:39.612264Z","shell.execute_reply":"2025-08-09T07:50:05.143481Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [144441344/144440600 00:02&lt;00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"imdb.vocab  README  test  tmp_clas  tmp_lm  train  unsup\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"files = get_text_files(path, folders = ['train', 'test', 'unsup'])\nfiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:12.920168Z","iopub.execute_input":"2025-08-09T07:50:12.920617Z","iopub.status.idle":"2025-08-09T07:50:13.681218Z","shell.execute_reply.started":"2025-08-09T07:50:12.920572Z","shell.execute_reply":"2025-08-09T07:50:13.680536Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(#100000) [Path('/root/.fastai/data/imdb/unsup/20881_0.txt'),Path('/root/.fastai/data/imdb/unsup/8703_0.txt'),Path('/root/.fastai/data/imdb/unsup/31794_0.txt'),Path('/root/.fastai/data/imdb/unsup/23935_0.txt'),Path('/root/.fastai/data/imdb/unsup/28007_0.txt'),Path('/root/.fastai/data/imdb/unsup/36578_0.txt'),Path('/root/.fastai/data/imdb/unsup/43724_0.txt'),Path('/root/.fastai/data/imdb/unsup/41715_0.txt'),Path('/root/.fastai/data/imdb/unsup/14380_0.txt'),Path('/root/.fastai/data/imdb/unsup/4692_0.txt'),Path('/root/.fastai/data/imdb/unsup/1518_0.txt'),Path('/root/.fastai/data/imdb/unsup/42552_0.txt'),Path('/root/.fastai/data/imdb/unsup/35811_0.txt'),Path('/root/.fastai/data/imdb/unsup/42921_0.txt'),Path('/root/.fastai/data/imdb/unsup/2234_0.txt'),Path('/root/.fastai/data/imdb/unsup/25734_0.txt'),Path('/root/.fastai/data/imdb/unsup/5802_0.txt'),Path('/root/.fastai/data/imdb/unsup/47089_0.txt'),Path('/root/.fastai/data/imdb/unsup/8415_0.txt'),Path('/root/.fastai/data/imdb/unsup/23558_0.txt')...]"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"txt = files[0].open().read()\ntxt, txt[:75]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:18.619414Z","iopub.execute_input":"2025-08-09T07:50:18.620034Z","iopub.status.idle":"2025-08-09T07:50:18.625931Z","shell.execute_reply.started":"2025-08-09T07:50:18.620007Z","shell.execute_reply":"2025-08-09T07:50:18.625003Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('Jiang Xian uses the complex backstory of Ling Ling and Mao Daobing to study Mao\\'s \"cultural revolution\" (1966-1976) at the village level. The film has the elements and pace of Chinese opera and so appears slow and sometimes sentimental to the foreign viewer. But the movie provides a window onto contemporary life in China, with its focus upon villagers in the city, the consuming quality of subsistence--daily struggle, family and local cruelties--and the appeal of movies as escape, fantasy, and, ultimately, as source of community. This last is the most radical element in the film, for it suggests the modern--and universal--experience of culture will replace the insular Chinese traditions. The child actors are particularly fine.',\n 'Jiang Xian uses the complex backstory of Ling Ling and Mao Daobing to study')"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**Word Tokenization**","metadata":{}},{"cell_type":"code","source":"spacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 50))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:24.303023Z","iopub.execute_input":"2025-08-09T07:50:24.303320Z","iopub.status.idle":"2025-08-09T07:50:27.577972Z","shell.execute_reply.started":"2025-08-09T07:50:24.303295Z","shell.execute_reply":"2025-08-09T07:50:27.577196Z"}},"outputs":[{"name":"stdout","text":"(#143) ['Jiang','Xian','uses','the','complex','backstory','of','Ling','Ling','and','Mao','Daobing','to','study','Mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-','1976',')','at','the','village','level','.','The','film','has','the','elements','and','pace','of','Chinese','opera','and','so','appears','slow','and','sometimes','sentimental','to','the','foreign'...]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"help(WordTokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:30.626939Z","iopub.execute_input":"2025-08-09T07:50:30.627928Z","iopub.status.idle":"2025-08-09T07:50:30.633852Z","shell.execute_reply.started":"2025-08-09T07:50:30.627893Z","shell.execute_reply":"2025-08-09T07:50:30.632852Z"}},"outputs":[{"name":"stdout","text":"Help on class SpacyTokenizer in module fastai.text.core:\n\nclass SpacyTokenizer(builtins.object)\n |  SpacyTokenizer(lang='en', special_toks=None, buf_sz=5000)\n |  \n |  Spacy tokenizer for `lang`\n |  \n |  Methods defined here:\n |  \n |  __call__(self, items)\n |      Call self as a function.\n |  \n |  __init__(self, lang='en', special_toks=None, buf_sz=5000)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 50))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:33.529555Z","iopub.execute_input":"2025-08-09T07:50:33.529827Z","iopub.status.idle":"2025-08-09T07:50:33.536405Z","shell.execute_reply.started":"2025-08-09T07:50:33.529804Z","shell.execute_reply":"2025-08-09T07:50:33.535683Z"}},"outputs":[{"name":"stdout","text":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to','study','xxmaj','mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-','1976',')','at','the','village','level','.','xxmaj','the','film','has','the','elements','and','pace','of','xxmaj','chinese','opera'...]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"help(Tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:36.196349Z","iopub.execute_input":"2025-08-09T07:50:36.196991Z","iopub.status.idle":"2025-08-09T07:50:36.204251Z","shell.execute_reply.started":"2025-08-09T07:50:36.196957Z","shell.execute_reply":"2025-08-09T07:50:36.203472Z"}},"outputs":[{"name":"stdout","text":"Help on class Tokenizer in module fastai.text.core:\n\nclass Tokenizer(fastcore.transform.Transform)\n |  Tokenizer(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' ')\n |  \n |  Provides a consistent `Transform` interface to tokenizers operating on `DataFrame`s and folders\n |  \n |  Method resolution order:\n |      Tokenizer\n |      fastcore.transform.Transform\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' ')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  decodes = (object,object) -> decodes\n |  \n |  encodes = (Path,object) -> encodes\n |  (str,object) -> encodes\n |  \n |  get_lengths(self, items)\n |  \n |  setups = (object,object) -> setups\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  from_df(text_cols, tok=None, rules=None, sep=' ', *, n_workers=4, mark_fields=None, tok_text_col='text', **kwargs)\n |  \n |  from_folder(path, tok=None, rules=None, *, extensions=None, folders=None, output_dir=None, skip_if_exists=True, output_names=None, n_workers=4, encoding='utf8', **kwargs)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __signature__ = <Signature (self, tok, rules=None, counter=None, lengt...\n |  \n |  input_types = (<class 'str'>, <class 'list'>, <class 'fastcore.foundat...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from fastcore.transform.Transform:\n |  \n |  __call__(self, x, **kwargs)\n |      Call self as a function.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  decode(self, x, **kwargs)\n |      Delegate to <code>decodes</code> to undo transform\n |  \n |  setup(self, items=None, train_setup=False)\n |      Delegate to <code>setups</code> to set up transform\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from fastcore.transform.Transform:\n |  \n |  name\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from fastcore.transform.Transform:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from fastcore.transform.Transform:\n |  \n |  init_enc = None\n |  \n |  order = 0\n |  \n |  split_idx = None\n |  \n |  train_setup = None\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Special Tokens**\n\nxxbos: Indicates the beginning of a text\n\nxxmaj: Indicates the next word begins with a capital\n\nxxunk: Indicates the next word is unknown","metadata":{}},{"cell_type":"code","source":"defaults.text_proc_rules","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:41.653804Z","iopub.execute_input":"2025-08-09T07:50:41.654062Z","iopub.status.idle":"2025-08-09T07:50:41.659460Z","shell.execute_reply.started":"2025-08-09T07:50:41.654042Z","shell.execute_reply":"2025-08-09T07:50:41.658652Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"##source code of the special token rules\n#??fix_html\n??spec_add_spaces","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:44.466824Z","iopub.execute_input":"2025-08-09T07:50:44.467507Z","iopub.status.idle":"2025-08-09T07:50:44.506198Z","shell.execute_reply.started":"2025-08-09T07:50:44.467475Z","shell.execute_reply":"2025-08-09T07:50:44.505485Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m \u001b[0mspec_add_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mSource:\u001b[0m   \n\u001b[0;32mdef\u001b[0m \u001b[0mspec_add_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m\"Add spaces around / and #\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_re_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.11/dist-packages/fastai/text/core.py\n\u001b[0;31mType:\u001b[0m      function\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"**Subword Tokenization**\n\nWord tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a\nsentence. However, this assumption is not always appropriate.","metadata":{}},{"cell_type":"code","source":"txts = L(o.open().read() for o in files[:2000])\n\n#txts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:47.770745Z","iopub.execute_input":"2025-08-09T07:50:47.771446Z","iopub.status.idle":"2025-08-09T07:50:47.867336Z","shell.execute_reply.started":"2025-08-09T07:50:47.771390Z","shell.execute_reply":"2025-08-09T07:50:47.866482Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#sz vocab size\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:54.617725Z","iopub.execute_input":"2025-08-09T07:50:54.617997Z","iopub.status.idle":"2025-08-09T07:50:54.622087Z","shell.execute_reply.started":"2025-08-09T07:50:54.617973Z","shell.execute_reply":"2025-08-09T07:50:54.621268Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"subword(1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:50:57.861711Z","iopub.execute_input":"2025-08-09T07:50:57.861973Z","iopub.status.idle":"2025-08-09T07:51:04.703812Z","shell.execute_reply.started":"2025-08-09T07:50:57.861950Z","shell.execute_reply":"2025-08-09T07:51:04.703071Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'▁J i ang ▁ X ian ▁us es ▁the ▁comp le x ▁back st or y ▁of ▁L ing ▁L ing ▁and ▁Ma o ▁Da o b ing ▁to ▁st u d y ▁Ma o \\' s ▁\" c ul'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"#For a smaller vocab, each token will represent fewer characters, and will require more tokens to represent a sentence\nsubword(300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:08.906930Z","iopub.execute_input":"2025-08-09T07:51:08.907217Z","iopub.status.idle":"2025-08-09T07:51:16.361968Z","shell.execute_reply.started":"2025-08-09T07:51:08.907194Z","shell.execute_reply":"2025-08-09T07:51:16.361224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'▁J i an g ▁ X i an ▁ us es ▁the ▁ com p le x ▁b a ck s t or y ▁of ▁L ing ▁L ing ▁and ▁M a o ▁D a o b ing ▁to ▁st'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#For a larger vocab, most common English words will end up in the vocab themselves, and will not need as many to represent a sentence:\n#A larger vocab means fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn.\nsubword(6000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:19.017596Z","iopub.execute_input":"2025-08-09T07:51:19.017858Z","iopub.status.idle":"2025-08-09T07:51:23.133768Z","shell.execute_reply.started":"2025-08-09T07:51:19.017838Z","shell.execute_reply":"2025-08-09T07:51:23.133010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'▁J ian g ▁X ian ▁uses ▁the ▁complex ▁back story ▁of ▁L ing ▁L ing ▁and ▁Ma o ▁Da ob ing ▁to ▁study ▁Ma o \\' s ▁\" cul t ural ▁revolution \" ▁( 1 9 6 6 - 1'"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"**Numericalization** (mapping tokens to integers)\n\ntoks = tkn(txt): Applies a tokenizer function *tkn* to the text 'txt'. The tokenizer splits the text into tokens, which are typically words or subwords. Here words are used.","metadata":{}},{"cell_type":"code","source":"toks = tkn(txt)\nprint(coll_repr(toks, 51))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:26.472786Z","iopub.execute_input":"2025-08-09T07:51:26.473052Z","iopub.status.idle":"2025-08-09T07:51:26.478462Z","shell.execute_reply.started":"2025-08-09T07:51:26.473031Z","shell.execute_reply":"2025-08-09T07:51:26.477637Z"}},"outputs":[{"name":"stdout","text":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to','study','xxmaj','mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-','1976',')','at','the','village','level','.','xxmaj','the','film','has','the','elements','and','pace','of','xxmaj','chinese','opera','and'...]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Tokenizing Multiple Texts\n\ntoks200 = txts[:200].map(tkn): Applying the tokenizer function *tkn* to the first 200 texts in the list 'txts'.\nThe map function is used to apply tkn to each text in the list txts[:200], resulting in a list of tokenized texts.\n","metadata":{}},{"cell_type":"code","source":"toks200 = txts[:200].map(tkn)\ntoks200[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:30.884447Z","iopub.execute_input":"2025-08-09T07:51:30.884706Z","iopub.status.idle":"2025-08-09T07:51:31.570031Z","shell.execute_reply.started":"2025-08-09T07:51:30.884687Z","shell.execute_reply":"2025-08-09T07:51:31.569315Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to'...]"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"Numericalization\n\nnum = Numericalize(): Initializes an instance of Numericalize, a class that converts tokens to numerical indices.\n\nnum.setup(toks200): Setting up up the vocabulary for numericalization using the tokenized texts in toks200. It creates a mapping from tokens to unique numerical indices.\n\ncoll_repr(num.vocab, 20): It prints a representation of the vocabulary created by num.setup. The vocab attribute of num is a dictionary mapping tokens to indices. The coll_repr function formats the vocabulary for printing, with 20 specifying the maximum number of vocabulary entries to display.\n\n","metadata":{}},{"cell_type":"code","source":"num = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:36.354388Z","iopub.execute_input":"2025-08-09T07:51:36.354680Z","iopub.status.idle":"2025-08-09T07:51:36.371868Z","shell.execute_reply.started":"2025-08-09T07:51:36.354658Z","shell.execute_reply":"2025-08-09T07:51:36.371181Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"\"(#2152) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"Converting Tokens to Numerical Indices\n\nnums = num(toks)[:20]: Converts the tokens in toks to numerical indices using the *num* object. The slice [:20] selects the first 20 numerical indices, and the result is stored in nums.\n\n","metadata":{}},{"cell_type":"code","source":"nums = num(toks)[:20]\nnums","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:39.939482Z","iopub.execute_input":"2025-08-09T07:51:39.939757Z","iopub.status.idle":"2025-08-09T07:51:39.956657Z","shell.execute_reply.started":"2025-08-09T07:51:39.939734Z","shell.execute_reply":"2025-08-09T07:51:39.955875Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TensorText([   2,    8,    0,    8,    0, 1269,    9, 1270,    0,   14,    8,    0,    8,    0,   12,    8,    0,    8,    0,   15])"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"Mapping Numerical Indices Back to Tokens","metadata":{}},{"cell_type":"code","source":"' '.join(num.vocab[o] for o in nums)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:43.434625Z","iopub.execute_input":"2025-08-09T07:51:43.435263Z","iopub.status.idle":"2025-08-09T07:51:43.442161Z","shell.execute_reply.started":"2025-08-09T07:51:43.435231Z","shell.execute_reply":"2025-08-09T07:51:43.441475Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"nums200 = toks200.map(num)\nnums200[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:45.728506Z","iopub.execute_input":"2025-08-09T07:51:45.729219Z","iopub.status.idle":"2025-08-09T07:51:45.786811Z","shell.execute_reply.started":"2025-08-09T07:51:45.729186Z","shell.execute_reply":"2025-08-09T07:51:45.785963Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TensorText([   2,    8,    0,    8,    0, 1269,    9, 1270,    0,   14,    8,    0,    8,    0,   12,    8,    0,    8,    0,   15, 1271,    8,    0,   22,   24,    0,  795,   24,   37,    0,   23,\n               0,   36,   48,    9,    0,  796,   11,    8,    9,   31,   60,    9, 1272,   12, 1273,   14,    8,    0,  902,   12,   54, 1274,  903,   12,  636,    0,   15,    9,    0,  500,   11,\n               8,   29,    9,   32,  797,   13,  713, 1062,    0,  128,   17,    8, 1591,   10,   27,  116,  904,  714,    0,   17,    9,  529,   10,    9,    0,  501,   14,    0,  186, 1275, 1276,\n              10,  309,   12,  637,    0,  186,   12,    9,  905,   14,  103,   26,  715,   10,  530,   10,   12,   10, 1063,   10,   26,    0,   14,    0,   11,    8,   21,  267,   16,    9,  107,\n               0,  906,   17,    9,   31,   10,   30,   18, 1592,    9,  907,  186,   12,  798,  186,  638,   14,  908,  104,    0,    9,    0,    8,    0,    0,   11,    8,    9, 1064,  237,   39,\n             716,  573,   11])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"dl = LMDataLoader(nums200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:49.276022Z","iopub.execute_input":"2025-08-09T07:51:49.276309Z","iopub.status.idle":"2025-08-09T07:51:49.283618Z","shell.execute_reply.started":"2025-08-09T07:51:49.276286Z","shell.execute_reply":"2025-08-09T07:51:49.282748Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"x,y = first(dl)\nx.shape,y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:52.710805Z","iopub.execute_input":"2025-08-09T07:51:52.711676Z","iopub.status.idle":"2025-08-09T07:51:52.748041Z","shell.execute_reply.started":"2025-08-09T07:51:52.711644Z","shell.execute_reply":"2025-08-09T07:51:52.747375Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(torch.Size([64, 72]), torch.Size([64, 72]))"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"#looking at the first row of the independent variable, which should be the start of the first text\n' '.join(num.vocab[o] for o in x[0][:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:51:56.467752Z","iopub.execute_input":"2025-08-09T07:51:56.468014Z","iopub.status.idle":"2025-08-09T07:51:56.475002Z","shell.execute_reply.started":"2025-08-09T07:51:56.467993Z","shell.execute_reply":"2025-08-09T07:51:56.474190Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to'"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"#The dependent variable is the same thing offset by one token\n' '.join(num.vocab[o] for o in y[0][:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:52:00.578506Z","iopub.execute_input":"2025-08-09T07:52:00.579107Z","iopub.status.idle":"2025-08-09T07:52:00.585970Z","shell.execute_reply.started":"2025-08-09T07:52:00.579080Z","shell.execute_reply":"2025-08-09T07:52:00.585112Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to study'"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"**Training**\n\nfastai handles tokenization and numericalization automatically when TextBlock is\npassed to DataBlock. All of the arguments that can be passed to Tokenizer and\nNumericalize can also be passed to TextBlock.","metadata":{}},{"cell_type":"code","source":"#using TextBlock in fastai defaults\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:52:05.034284Z","iopub.execute_input":"2025-08-09T07:52:05.035290Z","iopub.status.idle":"2025-08-09T07:55:41.240022Z","shell.execute_reply.started":"2025-08-09T07:52:05.035253Z","shell.execute_reply":"2025-08-09T07:55:41.239197Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"dls_lm.show_batch(max_n=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:55:42.105774Z","iopub.execute_input":"2025-08-09T07:55:42.106050Z","iopub.status.idle":"2025-08-09T07:55:42.455526Z","shell.execute_reply.started":"2025-08-09T07:55:42.106027Z","shell.execute_reply":"2025-08-09T07:55:42.454757Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos xxmaj if you want your vision of xxmaj chaplin limited to a lovable tramp and you get your belly laughs from pathos , watch something else . xxmaj if , however , you love slapstick comedy as performed by one of the best , do watch this one . \\n\\n xxmaj the image is of the tramp who really can not get the girl . xxmaj he spots another couple kissing on a park bench , and he has</td>\n      <td>xxmaj if you want your vision of xxmaj chaplin limited to a lovable tramp and you get your belly laughs from pathos , watch something else . xxmaj if , however , you love slapstick comedy as performed by one of the best , do watch this one . \\n\\n xxmaj the image is of the tramp who really can not get the girl . xxmaj he spots another couple kissing on a park bench , and he has a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>precisely because they were way to risky and would result in more losses from credit that is never repaid than profits from late fees interest . xxmaj but once the xxmaj federal xxmaj reserve drops the interest rates down to xxunk low levels , the risk changes and even those people who may not be ultimately able to pay beck their debt , can become profitable . xxmaj but xxmaj god forbid anyone should want to blame the xxmaj federal</td>\n      <td>because they were way to risky and would result in more losses from credit that is never repaid than profits from late fees interest . xxmaj but once the xxmaj federal xxmaj reserve drops the interest rates down to xxunk low levels , the risk changes and even those people who may not be ultimately able to pay beck their debt , can become profitable . xxmaj but xxmaj god forbid anyone should want to blame the xxmaj federal xxmaj</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"To convert the integer word indices into activations that we can use for our neural\nnetwork, *embeddings* will be used. Then those embeddings will be fed into a recurrent neural network (RNN),\nusing an architecture called AWD-LSTM.\n\nThe embeddings in the pretrained model are merged with random embeddings added for words that weren’t\nin the pretraining vocabulary. This is handled automatically inside *language_model_learner*.","metadata":{}},{"cell_type":"code","source":"learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3,\n        metrics=[accuracy, Perplexity()]).to_fp16()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:56:12.909730Z","iopub.execute_input":"2025-08-09T07:56:12.910462Z","iopub.status.idle":"2025-08-09T07:56:15.132437Z","shell.execute_reply.started":"2025-08-09T07:56:12.910402Z","shell.execute_reply":"2025-08-09T07:56:15.131686Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"learn.fit_one_cycle(1, 2e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T07:56:19.081862Z","iopub.execute_input":"2025-08-09T07:56:19.082733Z","iopub.status.idle":"2025-08-09T08:17:05.034130Z","shell.execute_reply.started":"2025-08-09T07:56:19.082698Z","shell.execute_reply":"2025-08-09T08:17:05.033345Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>4.004559</td>\n      <td>3.901768</td>\n      <td>0.300480</td>\n      <td>49.489857</td>\n      <td>20:45</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"#saving the state of your model\n\nlearn.save('1epoch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T08:17:15.119443Z","iopub.execute_input":"2025-08-09T08:17:15.119731Z","iopub.status.idle":"2025-08-09T08:17:16.008116Z","shell.execute_reply.started":"2025-08-09T08:17:15.119710Z","shell.execute_reply":"2025-08-09T08:17:16.007307Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Path('/root/.fastai/data/imdb/models/1epoch.pth')"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"#loading the model in another machine after creating your Learner the same way, or to resume training later\n#learn = learn.load('1epoch')\n\n#After initial training continuing fine-tuning the model after unfreezing\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T08:18:06.020288Z","iopub.execute_input":"2025-08-09T08:18:06.020598Z","iopub.status.idle":"2025-08-09T12:02:06.048465Z","shell.execute_reply.started":"2025-08-09T08:18:06.020575Z","shell.execute_reply":"2025-08-09T12:02:06.047785Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.763979</td>\n      <td>3.759217</td>\n      <td>0.316947</td>\n      <td>42.914803</td>\n      <td>22:24</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>3.696316</td>\n      <td>3.700335</td>\n      <td>0.323753</td>\n      <td>40.460838</td>\n      <td>22:19</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.644213</td>\n      <td>3.652735</td>\n      <td>0.329223</td>\n      <td>38.580021</td>\n      <td>22:19</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.563105</td>\n      <td>3.622987</td>\n      <td>0.332603</td>\n      <td>37.449265</td>\n      <td>22:18</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.490845</td>\n      <td>3.597589</td>\n      <td>0.335728</td>\n      <td>36.510101</td>\n      <td>22:24</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.427591</td>\n      <td>3.581691</td>\n      <td>0.338166</td>\n      <td>35.934254</td>\n      <td>22:25</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.368794</td>\n      <td>3.573433</td>\n      <td>0.339341</td>\n      <td>35.638737</td>\n      <td>22:27</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.295488</td>\n      <td>3.571903</td>\n      <td>0.340631</td>\n      <td>35.584255</td>\n      <td>22:15</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.238412</td>\n      <td>3.573689</td>\n      <td>0.340848</td>\n      <td>35.647846</td>\n      <td>22:29</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.223038</td>\n      <td>3.578161</td>\n      <td>0.340638</td>\n      <td>35.807613</td>\n      <td>22:35</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"*encoder* (The model not including the final layer) is saved. The final layer converts activations to probabilities of picking each token in our vocabulary.","metadata":{}},{"cell_type":"code","source":"learn.save_encoder('finetuned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:30:28.330108Z","iopub.execute_input":"2025-08-09T12:30:28.330951Z","iopub.status.idle":"2025-08-09T12:30:28.679769Z","shell.execute_reply.started":"2025-08-09T12:30:28.330918Z","shell.execute_reply":"2025-08-09T12:30:28.678912Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"**Text Generation** ","metadata":{}},{"cell_type":"code","source":"TEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n    for _ in range(N_SENTENCES)]\nprint(\"\\n\".join(preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:31:17.757945Z","iopub.execute_input":"2025-08-09T12:31:17.758221Z","iopub.status.idle":"2025-08-09T12:31:19.120530Z","shell.execute_reply.started":"2025-08-09T12:31:17.758199Z","shell.execute_reply":"2025-08-09T12:31:19.119761Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"name":"stdout","text":"i liked this movie because it was the horror movie for a small budget and every time i watched it i did n't think it was bad , i was pretty surprised by the low budget . This is a low budget movie that\ni liked this movie because it was the first Neil Gaiman movie i have ever seen . i would recommend this film to anyone looking for a good story and a good story . i also enjoyed the story about a \" mysterious\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"**Classifier fine-tuning**\n\nCreating the Classifier DataLoaders","metadata":{}},{"cell_type":"code","source":"dls_clas = DataBlock(blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n            get_y = parent_label,get_items=partial(get_text_files, folders=['train', 'test']),\n            splitter=GrandparentSplitter(valid_name='test')).dataloaders(path, path=path, bs=128, seq_len=72)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:33:47.564668Z","iopub.execute_input":"2025-08-09T12:33:47.565436Z","iopub.status.idle":"2025-08-09T12:33:50.949339Z","shell.execute_reply.started":"2025-08-09T12:33:47.565398Z","shell.execute_reply":"2025-08-09T12:33:50.948557Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"*vocab* of the language model is passed to ensure that it is using the same correspondence of token to index.\n\n'is_lm=False' (default option) ensures to the TextBlock that we have regular labeled data, rather than using the next tokens as\nlabels(as it was in the earlier fine-tuning model).\n","metadata":{}},{"cell_type":"code","source":"dls_clas.show_batch(max_n=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:33:53.745625Z","iopub.execute_input":"2025-08-09T12:33:53.746479Z","iopub.status.idle":"2025-08-09T12:33:54.040535Z","shell.execute_reply.started":"2025-08-09T12:33:53.746443Z","shell.execute_reply":"2025-08-09T12:33:54.039646Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n\\n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older adventure movies has been done well before , ( think _ the xxmaj dirty xxmaj dozen _ ) but _ atlantis _ represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>xxbos xxmaj warning : xxmaj does contain spoilers . \\n\\n xxmaj open xxmaj your xxmaj eyes \\n\\n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n't made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . \\n\\n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . \\n\\n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj</td>\n      <td>pos</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"#Creating mini-batches\n\nnums_samp = toks200[:10].map(num)\nnums_samp.map(len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:38:17.775224Z","iopub.execute_input":"2025-08-09T12:38:17.775955Z","iopub.status.idle":"2025-08-09T12:38:17.786549Z","shell.execute_reply.started":"2025-08-09T12:38:17.775923Z","shell.execute_reply":"2025-08-09T12:38:17.785732Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(#10) [158,319,181,193,114,145,260,146,252,295]"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"Creating model for text classification","metadata":{}},{"cell_type":"code","source":"learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,metrics=accuracy).to_fp16()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:40:01.878635Z","iopub.execute_input":"2025-08-09T12:40:01.878995Z","iopub.status.idle":"2025-08-09T12:40:03.620289Z","shell.execute_reply.started":"2025-08-09T12:40:01.878970Z","shell.execute_reply":"2025-08-09T12:40:03.619534Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"learn = learn.load_encoder('finetuned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:40:05.826528Z","iopub.execute_input":"2025-08-09T12:40:05.826782Z","iopub.status.idle":"2025-08-09T12:40:06.020212Z","shell.execute_reply.started":"2025-08-09T12:40:05.826764Z","shell.execute_reply":"2025-08-09T12:40:06.019526Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"Fine-Tuning the Classifier","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(1, 2e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:40:30.210962Z","iopub.execute_input":"2025-08-09T12:40:30.211648Z","iopub.status.idle":"2025-08-09T12:41:31.451296Z","shell.execute_reply.started":"2025-08-09T12:40:30.211617Z","shell.execute_reply":"2025-08-09T12:41:31.450318Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.246866</td>\n      <td>0.176477</td>\n      <td>0.931840</td>\n      <td>01:01</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:41:50.805511Z","iopub.execute_input":"2025-08-09T12:41:50.805821Z","iopub.status.idle":"2025-08-09T12:42:58.470649Z","shell.execute_reply.started":"2025-08-09T12:41:50.805794Z","shell.execute_reply":"2025-08-09T12:42:58.469624Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.218128</td>\n      <td>0.162630</td>\n      <td>0.938360</td>\n      <td>01:07</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:43:40.083712Z","iopub.execute_input":"2025-08-09T12:43:40.084046Z","iopub.status.idle":"2025-08-09T12:45:10.202948Z","shell.execute_reply.started":"2025-08-09T12:43:40.084015Z","shell.execute_reply":"2025-08-09T12:45:10.202118Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.195926</td>\n      <td>0.150040</td>\n      <td>0.944320</td>\n      <td>01:30</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:45:32.515830Z","iopub.execute_input":"2025-08-09T12:45:32.516623Z","iopub.status.idle":"2025-08-09T12:49:16.706402Z","shell.execute_reply.started":"2025-08-09T12:45:32.516583Z","shell.execute_reply":"2025-08-09T12:49:16.705541Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.160426</td>\n      <td>0.147225</td>\n      <td>0.945480</td>\n      <td>01:51</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.141472</td>\n      <td>0.146729</td>\n      <td>0.946240</td>\n      <td>01:52</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"learn.predict(\"I did not like the movie Avatar!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:49:49.932143Z","iopub.execute_input":"2025-08-09T12:49:49.932464Z","iopub.status.idle":"2025-08-09T12:49:49.970493Z","shell.execute_reply.started":"2025-08-09T12:49:49.932440Z","shell.execute_reply":"2025-08-09T12:49:49.969637Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"('neg', tensor(0), tensor([0.6537, 0.3463]))"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"##may not work with the keywords \nlearn.predict(\"I did not like Avatar!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T12:50:12.974839Z","iopub.execute_input":"2025-08-09T12:50:12.975146Z","iopub.status.idle":"2025-08-09T12:50:13.013348Z","shell.execute_reply.started":"2025-08-09T12:50:12.975114Z","shell.execute_reply":"2025-08-09T12:50:13.012581Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"('pos', tensor(1), tensor([0.4778, 0.5222]))"},"metadata":{}}],"execution_count":50}]}